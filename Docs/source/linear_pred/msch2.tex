%\input grafinp3
%\input psfig


\chapterA{Linear Stochastic  Dif\hbox{f}erence Equations\label{stodiff}}
\footnum=0
\section
{Introduction}

This chapter introduces the vector  first-order  linear stochastic
difference equation.\NFootnote{See Hansen and Sargent (2014) and Ljungqvist and Sargent (2012, ch.~2) for presentations of related material and extensions.}   \index{stochastic difference equation!linear first-order}%
  We  use it first to represent  information flowing to
economic agents, then again to represent competitive  equilibria.
The vector first-order linear stochastic difference equation is associated with a tidy
theory of prediction and a host of procedures for econometric application.
Ease of analysis has prompted us to adopt economic specifications
that cause competitive equilibria to have representations as vector first-order
linear stochastic difference equations.
\auth{Ljungqvist, Lars}%



Because it expresses next period's vector of state variables as a linear
function of this period's state vector and a vector of random disturbances, a vector first-order vector stochastic difference equation is {\it recursive\/}.  Disturbances that form a ``martingale difference sequence''  are  basic building blocks used to construct time series.
Martingale difference sequences are easy to forecast, a fact that
delivers convenient recursive formulas for optimal predictions of time series.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%\font\helvr=phvr at 8pt  %use in place of the original \tt style
%\font\helvrm=phvro at 8pt %for use in math mode
%
%
%\def\tim{\!\times\! }
%\def\no{\noindent}
%\def\bul{$\bullet$}
%\def\cir{$\circ$}
%\def\newpage{\vfill\eject}
%
%
%\def\sup#1{\line{\ninepoint\bf #1\hfill} \smallskip\hrule\bigskip}
%
%
%\def\purp{ \medskip\leftskip=30pt\no{\bf Purpose:}\par\leftskip=50pt }
%\def\syn{ \medskip\leftskip=30pt\no{\bf Synopsis:}\par\leftskip=50pt }
%\def\des{ \medskip\leftskip=30pt\no{\bf Description:}\par\leftskip=50pt }
%\def\alg{ \medskip\leftskip=30pt\no{\bf Algorithm:}\par\leftskip=50pt }
%\def\see{ \medskip\leftskip=30pt\no{\bf See also:}\par\leftskip=50pt }
%\def\ex{ \medskip\leftskip=30pt\no{\bf Examples:}\par\leftskip=50pt }
%\def\ref{ \medskip\leftskip=30pt\no{\bf References:}\par\leftskip=50pt }
%\def\li{ \medskip\leftskip=30pt\no{\bf Limitations:}\par\leftskip=50pt }
%%\def\ind#1{ \no #1\par}
%
%
%%-------------------------------aarma---------------------------------
%
%
%\sup{aarma}
%
%
%\purp \no
%Creates {\helvr arma} representation for a recursive linear equilibrium
%model.
%
%
%\syn \no
%{\helvr [num,den,p,z]=aarma(ao,c,sy,i)}
%
%
%\des \no
%The equilibrium is computed by first running {\helvr solvea}.  The
%equilibrium is
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5



\section
{Notation and Basic Assumptions}

Let $\{x_t : t=1,2,\ldots\}$ be a sequence of $n$-dimensional random
vectors, that is,  an $n$-dimensional stochastic process.  %The vector
%$x_t$ contains variables observed by economic agents at time $t$.
Let
$\{w_t : t=1,2,\ldots\}$ be a sequence of $N$-dimensional
random vectors.  We shall  express
$x_t$ as the sum of two terms. The first is
a moving average of past $w_t$'s. The second describes
the effects of an initial condition.  The $\{w_t\}$ generates
 a sequence of information sets $\{J_t : t=0,1,\ldots\}$.  Let
$J_0$ be generated by $x_0$ and $J_t$ be generated by $x_0, w_1, \ldots ,
w_t$, which means that $J_t$
consists of the set of all measurable functions of $\{x_0, w_1,\ldots,
w_t\}$.\NFootnote{The phrase ``$J_0$ is generated by $x_0$'' means that $J_0$ can
be expressed as a measurable function of $x_0$.}  The  process $\{w_{t+1}\}_{t=0}^\infty$ is
assumed to be a martingale difference sequence adapted to this sequence of
information sets.

\specsec
{Definition 1:} The sequence $\{w_t : t=1,2, \ldots\}$ is said to
be a {\it martingale difference sequence\/} \index{martingale difference sequence}
adapted to $\{J_t : t=0, 1, \ldots \}$
if $E(w_{t+1} \vert J_t) = 0$ for $t=0, 1, \ldots\,$.

\medskip\noindent
In addition, we assume that the  $\{w_t : t=1,2,\ldots\}$ process is {\it conditionally
homoscedastic}, a phrase whose meaning is conveyed by

\specsec
{Definition 2:} The sequence $\{w_t : t=1,2,\ldots\}$ is said to be
{\it conditionally homoscedastic\/} if $E(w_{t+1}w_{t+1}^\prime \mid J_t) = I$
for $t=0,1, \ldots\,$.

\medskip\noindent
It is an implication of the \idx{law of iterated expectations} that $\{w_t : t=1, 2,
\ldots\}$ is a sequence of (unconditional) mean zero, serially uncorrelated
random vectors.\NFootnote{Where $\phi_1$ and $\phi_2$ are information
sets with $\phi_1 \subset \phi_2$, and $x$ is a
random variable, the law of iterated expectations states that
$$E(x \mid \phi_1 ) = E (E (x \mid \phi_2) \mid \phi_1) . $$
Letting $\phi_1$ be the information set corresponding to no observations on any
random variables, letting $\phi_2 = J_t$, and applying this law to the process
$\{ w_t \}$, we obtain
$$ E(w_{t+1}) = E ( E ( w_{t+1} \mid J_t )) = E(0) = 0 .$$} %end note
 In addition, the entries of $w_t$ are assumed to be mutually uncorrelated.


The process $\{x_t : t=1,2,\ldots\}$ is constructed recursively using an
initial random vector $x_0$ and a time-invariant law of motion:
$$x_{t+1} = Ax_t + Cw_{t+1}\ ,\quad \hbox{for }\ t=0,1,\ldots ,\EQN ob1$$
where $A$ is an $n$ by $n$ matrix and $C$ is an $n$ by $N$ matrix.

Representation \Ep{ob1} will be a workhorse in this book.
 First, we will
use it to model
the information upon which economic
agents base their decisions.  Information will consist of
variables that drive shocks to preferences and to technologies.
Second, we shall specify the economic problems faced by
the agents in our models and the economic arrangement through which agents'
decisions are reconciled (competitive equilibrium) so that
the state of the economy has a representation of the form \Ep{ob1}.

\section
{Prediction Theory} \index{prediction theory!linear difference equations}%
A tractable theory of prediction is associated with \Ep{ob1}. We use this theory
extensively both in computing a competitive  equilibrium and in
representing that equilibrium in the form of \Ep{ob1}.
The optimal forecast of $x_{t+1}$ given current information is
$$ E(x_{t+1} \mid J_t) = Ax_t , \EQN ob2 $$
and the one-step-ahead forecast error is
$$x_{t+1} - E(x_{t+1} \mid J_t) = Cw_{t+1} . \EQN ob3 $$
The covariance matrix of $x_{t+1}$ conditioned on $J_t$ is % just $CC^\prime$:
$$E (x_{t+1} - E ( x_{t+1} \mid J_t) ) (x_{t+1} - E ( x_{t+1} \mid J_t))^\prime
= CC^\prime . \EQN ob4 $$

A nonrecursive expression for $x_t$ as a function of
$x_0, w_1, w_2, \ldots, \allowbreak w_t$ can be found by using \Ep{ob1} repeatedly to obtain
\offparens
$$ \eqalign{ x_t & = Ax_{t-1} + Cw_t \cr
& = A^2 x_{t-2} + ACw_{t-1} + Cw_t \cr
& = \Bigl[\sum_{\tau=0}^{t-1} A^\tau Cw_{t-\tau} \Bigr] + A^t x_0 . \cr}
\EQN ob5$$
\auth{Slutsky, Eugen}\auth{Sims, Christopher A.}%
\index{impulse response}%
Representation \Ep{ob5} is one type of {\it moving-average\/} representation.
\index{moving average!representation}%
  It expresses $\{x_t : t=1,2,\ldots\}$
as a linear function of current and past values of the  process
$\{w_t: t=1,2,\ldots\}$ and an initial condition $x_0$.\NFootnote{Slutsky (1937) argued
that business cycle fluctuations could be approximated by moving average
processes.   Sims (1980) showed that a fruitful way to summarize correlations
between time series is to calculate an impulse response function.
In chapter \use{kfilter}, we study the relationship between
the impulse response functions based on the {\it vector autoregressions}  calculated by Sims (1980) and
the impulse response function associated with \Ep{ob5}.}  The list of moving average coefficients
$\{ A^\tau C: \tau=0,1, \ldots\}$ in  representation \Ep{ob5} is often
called an {\it impulse response function}.  An impulse response function
depicts the response of current and future values of $\{x_t\}$ to
a random shock $w_t$.
In representation \Ep{ob5}, the impulse response function is given
by entries of the vector sequence $\{ A^\tau C: \tau=0,1, \ldots\}$.\NFootnote{Given
matrices $A$ and $C$, the impulse response function can be calculated using
the MATLAB program {\tt dimpulse.m.} }

\mtlb{dimpulse.m}

Shift \Ep{ob5} forward in time:
$$x_{t+j} = \sum^{j-1}_{s=0} A^s C w_{t+j-s} + A^j x_t . \EQN ob6 $$
Projecting both sides of \Ep{ob6} on the information set $\{ x_0, w_t, w_{t-1},
\ldots, w_1\}$ gives\NFootnote{For an elementary discussion of linear
least squares projections, see Sargent (1987b, ch.~11).}
$$ E_t x_{t+j} = A^j x_t . \EQN ob7 $$
where $E_t (\cdot) \equiv  E [ (\cdot) \mid x_0, w_t, w_{t-1}, \ldots, w_1]
= E (\cdot) \mid J_t$, and $x_t$ is in $J_t$. Equation \Ep{ob7}
gives the optimal $j$-step-ahead prediction.


It is useful to obtain the covariance matrix of the $j$-step-ahead prediction
error
$$x_{t+j} - E_t x_{t+j} = \sum^{j-1}_{s=0} A^s C w_{t-s+j}. \EQN ob8 $$
Evidently,
$$\eqalign{E_t (x_{t+j} - E_t x_{t+j})\ & (x_{t+j} - E_t x_{t+j})^\prime\cr
&= \sum^{j-1}_{k=0} A^k C C^\prime A^{k^\prime} \equiv v_j . \cr } \EQN
ob9;a $$
Note that $v_j$ defined in \Ep{ob9;a} can be calculated recursively via
$$\eqalign{v_1 &= CC^\prime \cr
v_j &= CC^\prime + A v_{j-1} A^\prime, \quad j \geq 2 . \cr}\EQN ob9;b $$
For $j \geq 1$,  $v_j$ is the conditional covariance matrix of the errors in forecasting
$x_{t+j}$ on the basis of time $t$ information $x_t$.  To decompose these
covariances into parts attributable to the individual components of $w_t$, we
let $i_\tau$ be an $N$-dimensional column vector of zeroes except in position
$\tau$, where there is a one.  Define a matrix $\upsilon_{j,\tau}$ by
$$ \upsilon_{j,\tau} = \sum_{k=0}^{j-1} A^k C i_\tau i_\tau^\prime C^\prime
A^{^\prime k} . \EQN ob9;c $$
Note that $\sum_{\tau=1}^N i_\tau i_\tau^\prime = I$, so that from
\Ep{ob9;a} and \Ep{ob9;c} we have
$$\sum_{\tau=1}^N \upsilon_{j, \tau} = \upsilon_j . $$
Evidently, the matrices $\{ \upsilon_{j, \tau} , \tau = 1, \ldots, N \}$ give
an orthogonal decomposition of the covariance matrix of $j$-step-ahead
prediction errors into the parts attributable to each of the components $\tau =
1, \ldots, N$.\NFootnote{For given matrices $A$ and $C$, the
matrices $v_{j, \tau}$
and $v_j$ are calculated by the MATLAB program {\tt evardec.m.} }
\mtlb{evardec.m}

The ``\idx{innovation accounting}'' methods of Sims (1980) are based on \Ep{ob9}.
Sims recommends computing the matrices $v_{j,\tau}$ in \Ep{ob9} for a
sequence $j= 0,1,2, \ldots$. This sequence represents
the effects of components of the shock process $w_t$ on the
covariance of $j$-step-ahead prediction errors for each series in
$x_t$.

\section
{Transforming Variables to Uncouple Dynamics}

It is sometimes useful  to
uncouple the  dynamics of $x_t$ by
 using the distinct eigenvalues of the matrix $A$.  The Jordan decomposition of the matrix $A$ is
$$A = TDT^{-1} , \EQN ob10 $$
where $T$ is a nonsingular matrix and $D$ is another  matrix to be constructed.
The eigenvalues of $A$ are the zeroes of the polynomial $\det\,
(\zeta I - A)$.  This polynomial has $n$
zeroes because $A$ is $n$ by $n$.  Not all of these zeroes are necessarily
distinct.\NFootnote{In the case in which the eigenvalues of $A$
are distinct, $D$ is taken to be the diagonal matrix whose entries are
the eigenvalues and $T$ is the matrix of eigenvectors corresponding
to those eigenvalues.} Suppose that there are $m \leq n$ distinct zeroes
of this polynomial,
denoted $\delta_1, \delta_2, \ldots, \delta_m$.  For each $\delta_j$, we
construct a matrix $D_j$ that has the same dimension as the number of zeroes
of $\det\, (\zeta I - A)$ that  equal $\delta_j$.  The diagonal entries
of $D_j$ are $\delta_j$ and the entries in the single diagonal row above the
main diagonal are all either zero or one.  The remaining entries of $D_j$ are
zero.  Then the matrix $D$ is block diagonal with $D_j$ in the
$j^{\rm th}$ diagonal block.

Transform the state vector $x_t$ as follows:
$$ x_t^\ast = T^{-1} x_t . \EQN ob11 $$
Substituting into \Ep{ob1} gives
$$x_{t+1}^\ast = Dx_t^\ast + T^{-1} Cw_{t+1} . \EQN ob12 $$
Since $D$ is block diagonal, we can partition $x_t^\ast$ according to the
diagonal blocks of $D$ or, equivalently, according to the distinct eigenvalues
of $A$.  In the law of motion \Ep{ob12}, partition $j$ of $x_{t+1}^\ast$
is linked only to partition $j$ of $x_t^\ast$.  In this sense, the dynamics
of system \Ep{ob12} are {\it uncoupled}.  To calculate multiperiod
forecasts and dynamic multipliers, we must raise the matrix $A$ to
integer powers (see \Ep{ob7}).  It is straightforward to verify that
$$ A^\tau = T(D^\tau)T^{-1} . \EQN ob13$$
Since $D$ is block diagonal, $D^\tau$ is also block diagonal, where block $j$
is just $(D_j)^\tau$.  The matrix $(D_j)^\tau$ is upper triangular with
$\delta_j^\tau$ on the diagonal, with all entries of the $k^{\rm th}$ upper
right diagonal given by
$$(\delta_j)^{\tau-k}\ \tau!/[k!(\tau-k)!] \ \hbox{ for } \ 0 \leq k \leq
\tau , \EQN ob14 $$
and zeroes elsewhere.  Consequently, raising $D$ to an integer power involves
raising the eigenvalues to integer powers.  Some of the eigenvalues of $A$ may
be complex.  In this case, it is convenient to use the polar decomposition of
the eigenvalues.  Write eigenvalue $\delta_j$ in polar form as
$$\delta_j = \rho_j \exp (i \theta_j) = \rho_j [\cos (\theta_j) + i \sin
(\theta_j)]  \EQN ob15 $$
where $\rho_j = \mid \delta_j \mid$.  Then
$$ \delta_j^\tau = ( \rho_j)^\tau \exp (i \tau \theta_j) = (\rho_j)^\tau
[\cos (\tau \theta_j) + i \sin (\tau \theta_j)] . \EQN ob16 $$
We shall often assume that $\rho_j$ is less than or equal to one, which rules
out instability in the dynamics.  Whenever $\rho_j$ is strictly less than one,
the term $(\rho_j)^\tau$ decays to zero as $\tau\to\infty$.  When $\theta_j$
is different from zero, eigenvalue $j$ induces an oscillatory component
with period $(2 \pi / \mid \theta_j \mid)$. %Thus, using equation \Ep{ob16} allows us
%to write
%$$ A^\tau = T {\rm diag} \left( \rho_j^\tau[\cos(\tau \theta_j) + i \sin (\tau \theta_j) ]\right) T^{-1} .$$ % \cr
%%       & = T {\rm diag} \left(\rho_j^\tau \cos (\tau \theta_j)\right) T^{-1} + i T {\rm diag} \left(\rho_j^\tau \sin (\tau \theta_j)\right) T^{-1} \cr
%       & =
%\section


Next we consider some examples of processes that can be represented as \Ep{ob1}.

\subsection
{Deterministic Seasonals}

We can use \Ep{ob1} to represent $y_t = y_{t-4}$.  Let $n=4, C=0$,
$x_t = (y_t, y_{t-1} , y_{t-2} , y_{t-3})^\prime, x_0 = (0 \ 0 \ 0 \
1)^\prime$,
$$A = \left[ \matrix
{ 0 & 0 & 0 & 1 \cr
1 & 0 & 0 & 0 \cr
0 & 1 & 0 & 0 \cr
0 & 0 & 1 & 0 \cr } \right] , \quad C =  \left[ \matrix { 0 \cr 0 \cr 0
\cr 0 \cr } \right] . \EQN ob17 $$
Here the $A$ matrix has four distinct eigenvalues and the absolute
values of each of these eigenvalues is one.  Two eigenvalues are real $(1,-1)$
and two  are imaginary $(i,-i)$,  and so have period four.\NFootnote{For example, note that from representation
\Ep{ob15}, $i = \exp\left(\pi/2 \right) + i \sin \left(\pi/2\right)$, so  the period associated
with $i$ is ${\frac{2 \pi}{\frac{\pi}{2}}} = 4$.} The
resulting sequence $\{x_t : t=1,2,\ldots\}$ oscillates deterministically
with period four.   It can be used to model deterministic seasonals in
quarterly time series.

\subsection
{Indeterministic Seasonals}

We want to use \Ep{ob1} to represent
$$ y_t = \alpha_4 y_{t-4} + w_t , \EQN ob18 $$
where $w_t$ is a martingale difference sequence and $\mid \alpha_4\mid \leq 1$.
We define $x_t = [y_t,\ y_{t-1},\ y_{t-2},\ y_{t-3} ]^\prime,\ n = 4 $,
$$A = \left[ \matrix { 0 & 0 & 0 & \alpha_4 \cr 1 & 0 & 0 & 0 \cr
0 & 1 & 0 & 0 \cr 0 & 0 & 1 & 0 \cr } \right]  \ , \ C = \left[ \matrix
{ 1 \cr 0 \cr 0 \cr 0 \cr } \right] . $$
With these definitions, \Ep{ob1} represents \Ep{ob18}. This model displays an
``indeterministic'' seasonal.  Realizations of \Ep{ob18} display recurrent,
but aperiodic, seasonal fluctuations.

\subsection
{Univariate Autoregressive Processes}

We can use \Ep{ob1} to represent the model
$$ y_t = \alpha_1 y_{t-1} + \alpha_2 y_{t-2} + \alpha_3 y_{t-3} + \alpha_4
y_{t-4} + w_t ,\EQN ob19 $$
where $w_t$ is a martingale difference sequence.  We set $n = 4, x_t = [y_t \
y_{t-1} \  y_{t-2} \  y_{t-3} ]^\prime$,
$$A = \left[ \matrix{ \alpha_1 & \alpha_2 & \alpha_3 &
\alpha_4 \cr 1 & 0 & 0 & 0 \cr 0 & 1 & 0 & 0 \cr 0 & 0 & 1 & 0 \cr }
\right]  \ , \ C = \left[ \matrix { 1 \cr 0 \cr 0 \cr 0 \cr } \right] . $$
The matrix $A$ has the form of the {\it companion matrix\/} to the vector
\hfil \break
$[\alpha_1 \  \alpha_2 \ \alpha_3 \ \alpha_4]$.

\subsection
{Vector Autoregressions}

Reinterpret \Ep{ob19} \index{vector autoregression}
as a vector process in which $y_t$ is a $(k \times 1)$
vector, $\alpha_j$ a $(k \times k)$ matrix, and $w_t$ a $k \times 1$ martingale
difference sequence.  Then \Ep{ob19} is termed a {\it vector autoregression}.
To map this into \Ep{ob1}, we set $n = k \cdot 4$,
$$ A = \left[ \matrix { \alpha_1 & \alpha_2 & \alpha_3 & \alpha_4 \cr
                               I & 0        & 0        & 0        \cr
                               0 & I        & 0        & 0        \cr
                               0 & 0        & I        & 0        \cr }\right],
\ C = \left[ \matrix { I \cr 0 \cr 0 \cr 0 \cr } \right] $$
where $I$ is the $(k \times k)$ identity matrix.

\subsection
{Polynomial Time Trends}

Let $n=2,x_0 = [0 \ 1]^\prime$, and
$$ A = \left[ \matrix { 1 & 1  \cr 0 & 1  \cr } \right] , \ \  C =
\left[ \matrix { 0 \cr 0 \cr } \right]. \EQN ob20  $$
Notice that $D=A$ in the Jordan decomposition of $A$.  It follows from
\Ep{ob14} that
$$ A^t = \left[ \matrix { 1 & t  \cr 0 & 1  \cr } \right] . \EQN ob21 $$
Hence $x_t = \bmatrix{ t &1}^\prime$, so that the first component of $x_t$ is a linear
time trend and the second component is a constant.

It is also possible to use \Ep{ob1} to represent polynomial trends of any order.
For instance, let $n=3,C=0,x_0 = \bmatrix{0 &0 &1}^\prime$, and
$$ A = \left[ \matrix { 1 & 1 & 0  \cr 0 & 1 & 1  \cr 0 & 0 & 1 \cr} \right] .
\EQN ob22 $$
Again, $A=D$ in the Jordan decomposition of $A$.  It follows from \Ep{ob14}
that
$$A^t = \left[ \matrix { 1 & t & t(t-1)/2 \cr 0 & 1 & t \cr 0 & 0 & 1 \cr }
\right] . \EQN ob23 $$
Then $x_t^\prime = \bmatrix{t(t-1)/2 &t & 1}$, so that $x_t$ contains  linear and
quadratic time trends.

\subsection
{Martingales with Drift}

We modify the linear time trend example by making $C$ nonzero.  Suppose
that $N$ is one and $C^\prime = [1 \ 0]$.  Since $A = \left[ \matrix { 1
& 1 \cr 0 & 1 \cr } \right] $ and $ A^t = \left[ \matrix { 1 & t \cr 0
& 1 \cr } \right] $, it follows that
$$A^\tau C = \left[ \matrix { 1 \cr 0 \cr} \right] . \EQN ob24$$
Substituting into the moving-average representation \Ep{ob5}, we obtain (2.25)
$$ x_{1t} = \sum_{\tau=0}^{t-1} w_{t-\tau} + [1 \ t] x_0 $$
where $x_{1t}$ is the first entry of $x_t$.  The first term on the right
side of the preceding equation  is a cumulated sum of martingale differences,
and is called a {\it martingale}, while the second term is a
translated linear function of time.


\subsection
{Covariance Stationary Processes}

Next we consider specifications of $x_0$ and $A$ that imply that the first
two unconditional moments of $\{x_t : t=1,2,\ldots\}$ are replicated over time. Let $A$
satisfy
$$A = \left[\matrix{A_{11} & A_{12} \cr 0 & 1\cr } \right], \EQN ob26 $$
where $A_{11}$ is an $(n-1) \times (n-1)$ matrix with eigenvalues that have
moduli strictly less than one and $A_{12}$ is an $(n-1) \times 1$
column vector.  In addition, let $C^\prime = [C_1^\prime \ 0]$.  We partition
$x_t^\prime = [x_{1t}^\prime\ x_{2t}^\prime]$,  where $x_{1t}$ has $n-1$ entries.
It follows from \Ep{ob1} that
$$x_{1t+1} = A_{11}x_{1t} + A_{12}x_{2t} + C_{1}w_{t+1} \EQN ob27 $$
$$ x_{2t+1} = x_{2t} . \EQN ob28 $$
By construction, the second component, $x_{2t}$, simply replicates itself
over time.  For convenience, take $x_{20} = 1$ so that $x_{2t} = 1$ for
$t=1,2,\ldots $.


We can use \Ep{ob27} to compute the first two moments of $x_{1t}$.  Let
$\mu_t = Ex_{1t}$.  Taking unconditional expectations on both sides of
\Ep{ob27} gives
$$\mu_{t+1} = A_{11} \mu_t + A_{12}  . \EQN ob29 $$
We can solve the nonstochastic difference equation \Ep{ob29} for the
stationary value of $\mu_t$.  Define $\mu$ as the stationary value of
$\mu_t$, and substitute $\mu$ for $\mu_t$ and $\mu_{t+1}$ in \Ep{ob29}.
Solving for $\mu$ gives $\mu = (I-A_{11})^{-1} A_{12} $.  Therefore, if
$$ Ex_{10} = (I-A_{11})^{-1} A_{12} , \EQN ob30 $$
then $Ex_{1t}$ will be constant over time and equal to the value on the
right side of \Ep{ob30}.  Furthermore, if the eigenvalues of $A_{11}$ are
less than unity in modulus, then starting from any initial value of
$\mu_0$, $\mu_t$ will converge to the stationary value $(I-A_{11})^{-1}
A_{12}$.

Next we use \Ep{ob27} to compute the unconditional covariances of $x_t$.
Subtracting \Ep{ob29} from \Ep{ob27} gives
$$(x_{1t+1} - \mu_{t+1}) = A_{11} (x_{1t} - \mu_t) + C_1 w_{t+1} .\EQN
ob31$$
From \Ep{ob31} it follows that
$$ \eqalign { (x_{1t+1} - \mu_{t+1}) (x_{1t+1} - \mu_{t+1})^\prime  & = A_{11}
(x_{1t} - \mu_t) (x_{1t} - \mu_t)^\prime A_{11}^\prime  + C_1 w_{t+1} w_{t+1}^\prime C_1^\prime
 \cr  &          + C_1 w_{t+1} (x_{1t} - \mu_t )^\prime A_{11}^\prime
          + A_{11} (x_{1t} - \mu_t) w_{t+1}^\prime C_1^\prime  .  } $$
The \idx{law of iterated expectations} implies that $w_{t+1}$ is orthogonal to
$(x_{1t} - \mu_t)$.  Therefore, taking expectations on both sides of
the above equation gives
$$ V_{t+1} = A_{11} V_t A_{11}^\prime + C_1 C_1^\prime  , $$
where $V_t \equiv E(x_{1t} - \mu_t) (x_{1t} - \mu_t)^\prime$.  Evidently,
the stationary value $V$ of the covariance matrix $V_t$ must satisfy
$$ V = A_{11} V A_{11}^\prime + C_1 C_1^\prime. \EQN ob32 $$
It is straightforward to verify that $V$ is a solution of \Ep{ob32} if
and only if
$$ V = \sum^\infty _{j=0} A_{11}^j C_1 C_1^\prime A_{11}^{j \prime} .
\EQN ob33 $$
The infinite sum \Ep{ob33} converges when all eigenvalues
of $A_{11}$ are less in modulus than unity.\NFootnote{Equation
\Ep{ob32} is known as the discrete Lyapunov equation. \index{Lyapunov equation!discrete}%
Given the matrices $A_{11}$ and $C_1$, this equation is solved by the MATLAB
programs {\tt dlyap.m} and {\tt doublej.m}.} \mtlb{doublej.m}\mtlb{dlyap.m}%
  If the covariance matrix of
$x_{10}$ is $V$ and the mean of $x_{10}$ is $(I-A_{11})^{-1} A_{12}$,
then the covariance and mean of $x_{1t}$ remain constant over time.
In this case, the process is said to be
{\it covariance stationary}. \index{covariance stationarity}%
If the eigenvalues of $A_{11}$ are all less than unity in modulus,
then $V_t \to V$ as $t \to \infty$, starting from any initial value $V_0$.

From \Ep{ob9} and \Ep{ob33}, notice that if all eigenvalues of
$A_{11}$ are less than unity in modulus, then $\lim_{j \to \infty} v_j =
V$.  That is, the covariance matrix of $j$-step-ahead forecast
errors converges to the unconditional covariance
matrix of $x$ as the horizon $j$ goes to infinity.\NFootnote{The doubling
algorithm described in chapter \use{compute} can be used to compute
the solution of \Ep{ob32} via iterations that approximate \Ep{ob33}.
\mtlb{doublej2.m}\mtlb{doublej.m}%
  The algorithm
is implemented in the MATLAB programs {\tt doublej.m} and {\tt doublej2.m}.}

The matrix $V$ can be decomposed to display  contributions of each
entry of the process $\{ w_t \}$.  Let $\iota_\tau$ be an $N$-dimensional
column vector of zeroes except in position $\tau$, where there is a one.  Then
$$I = \sum_{\tau=1}^N\, \iota_\tau \iota_\tau^\prime .  \EQN ob34 $$
Define % a matrix $\tilde V_\tau$
$$\tilde V_\tau \equiv \sum_{j=o}^\infty\, (A_{11})^j C_1 \iota_\tau
\iota_\tau^\prime C_1^\prime (A_{11})^{j \prime}. \EQN  ob35 $$
We have, by analogy to \Ep{ob32} and \Ep{ob33}, that $\tilde V_\tau$ satisfies
$\tilde V_\tau = A_{11} \tilde V_\tau A_{11}^\prime + C_1 i_\tau i_\tau^\prime
C_1^\prime$. In light of \Ep{ob34}, \Ep{ob35}, and \Ep{ob33}, we have that
$$V= \sum_{\tau=1}^N \, \tilde V_\tau . \EQN  ob36 $$
The matrix $\tilde V_\tau$ is the contribution to
$V$ of the $\tau^{\rm th}$ component of the process $\{w_t : t=1,2,\ldots\}$.
Hence, \Ep{ob36} gives a decomposition of the covariance matrix $V$ into parts
attributable to each of the underlying economic shocks.

Next, consider the autocovariances of $\{x_t : t=1,2,\ldots\}$.  From the
\idx{law of iterated expectations}, it follows that
$$ \eqalign{ E[(x_{1t+\tau}- \mu)(x_{1t} - \mu)^\prime] & = E\{ E[(x_{1t+\tau}
- \mu) \mid J_t] (x_{1t} - \mu)^\prime \} \cr
& = E[A_{11}^\tau (x_{1t} - \mu)(x_{1t} - \mu)^\prime] \cr
& = A_{11}^\tau V . \cr }  \EQN ob37 $$
Notice that this expected cross-product or {\it autocovariance\/} does not
depend on calendar time,  only on the gap $\tau$ between the time
indices.\NFootnote{Equation \Ep{ob37} shows
that the matrix autocovariogram of $x_{1t}$ (i.e., $\Gamma_\tau \equiv
E [ ( x_{1t + \tau} - \mu ) (x_{1t} - \mu)^\prime]$ taken as a function
of $\tau$) satisfies the nonrandom difference equation $\Gamma_{t+1} =
A_{11} \Gamma_t$.}
Independence from calendar time of means, covariances, and
autocovariances  defines
{\it covariance stationary\/} processes.
  For the particular class of processes we
are considering, if the covariance matrix does not depend on calendar time,
then none of the autocovariance matrices does.

\subsection{Multivariate ARMA Processes}
\index{arma!autoregressive, moving average processes}%
Specification \Ep{ob1} assumes that $x_t$ contains all the information
available at time $t$ to forecast $x_{t+1}$.  In many applications, vector time
series are modeled as multivariate autoregressive moving-average (ARMA)
processes.  Let $y_t$ be a vector stochastic process.  An ARMA process $\{y_t :
t=1,2,\ldots\}$ has a representation
$$\eqalign{ y_t = \alpha_1 y_{t-1} &+ \alpha_2 y_{t-2} + \cdots + \alpha_k
y_{t-k} \cr
&+ \gamma_0 w_t + \gamma_1 w_{t-1} + \cdots + \gamma_k w_{t-k} \cr} \EQN
ob38 $$
where $E[w_t \mid y_{t-1}, y_{t-2}, \cdots y_{t-k+1}, w_{t-1}, w_{t-2}, \cdots
w_{t-k+1} ] = 0$.  The requirement that the same number of lags of $y$ enter
\Ep{ob38} as the number of lags of $w$ is not restrictive because some
coefficients can be set to zero.  Hence, we can think of $k$ as being the
greater of the two lag lengths.  A representation such as \Ep{ob38} can
be shown to satisfy \Ep{ob1}.  To see this, we define
%

{\eightpoint
$$\eqalign{&x_t = \cr
&\left[\matrix{
y_t \cr
\alpha_2 y_{t-1} +\alpha_3 y_{t-2} \cdots +\alpha_k y_{t-k+1} + \gamma_1 w_t +
\gamma_2 w_{t-1} \cdots +\gamma_{k-1} w_{t-k+2}+\gamma_k w_{t-k+1} \cr
\alpha_3 y_{t-1} \cdots + \alpha_k y_{t-k+2} + \gamma_2 w_t \cdots +
\gamma_{k-1} w_{t-k+3} +\gamma_k w_{t-k+2} \cr
\vdots \cr
\alpha_k y_{t-1} + \gamma_{k-1} w_t +\gamma_k w _{t-1}\cr
\gamma_k w_t \cr} \right]\cr} \EQN ob40new $$
}
$$ C = \left[\matrix {\gamma_0 \cr \gamma_1  \cr \vdots \cr
\gamma_k \cr} \right] \EQN ob40  $$
and
$$ A = \left[ \matrix {
\alpha_1 & I & \cdots & 0 \cr
\alpha_2 & 0 & \cdots & 0 \cr
\vdots & \vdots & \ddots & \vdots \cr
\alpha_k & 0 & \cdots & I \cr
0 & 0 & \cdots & 0 \cr } \right] \EQN ob41  $$
It is straightforward to verify that the resulting process $\{x_t : t=1,2,
\ldots\}$ satisfies \Ep{ob1}.

\subsection
{Prediction of a Univariate First-Order ARMA}

Consider the special case of \Ep{ob38}
$$ y_t = \alpha_1 y_{t-1} + \gamma_0 w_t + \gamma_1 w_{t-1} \EQN ob42 $$
where $y_t$ is a scalar stochastic process and $w_t$ is a scalar white noise.
Assume that $\mid \alpha_1 \mid < 1 $ and that $ \mid \gamma_1 / \gamma_0 \mid
< 1$.  Applying \Ep{ob40new}, we define the state $x_t$ as
%
%\beginleftbox
%Tom, (2.39) referred to but no original equation with that number.\endleftbox


$$x_t = \left[\matrix { y_t \cr \gamma_1 w_t \cr } \right] . $$
Applying \Ep{ob40} and \Ep{ob41}, we have
$$ C = \left[ \matrix { \gamma_0 \cr \gamma_1 \cr } \right] ,\quad A =
\left[ \matrix { \alpha_1 & 1 \cr 0 & 0  \cr } \right] . $$
We can apply \Ep{ob7} to obtain a formula for the optimal $j$-step-ahead
prediction of $y_t$.  Using \Ep{ob7} in the present example gives
$$E_t \left[ \matrix { y_{t+j} \cr \gamma_1 w_{t+j} \cr } \right] \
= \left[ \matrix{ \alpha_1^j & \alpha_1^{j-1} \cr 0 & 0 \cr } \right]\
\left[\matrix{ y_t \cr \gamma_1 w_t \cr}\right]$$
which implies that
$$E_t y_{t+j} = \alpha_1^j y_t + \alpha_1^{j-1} \gamma_1 w_t . \EQN ob43 $$
We can use \Ep{ob43} to derive a famous formula of John F. Muth (1960).
\auth{Muth, John F.}%
  Assume that the
system \Ep{ob42} has been operating forever, so that the initial time is
infinitely far in the past.  Then using the lag operator $L$, express
 \Ep{ob42} as
$$(1 - \alpha_1 L) y_t = (\gamma_0 + \gamma_1 L) w_t . $$
Solving for $w_t$ gives
$$ w_t = \gamma_0^{-1} \Bigl( {1 - \alpha_1 L \over 1 + {\gamma_1
\over \gamma_0} L} \Bigr)\, y_t , $$
which expresses $w_t$ as a geometric distributed lag of current and past
$y_t$'s.  Substituting this expression for $w_t$ into \Ep{ob43} and rearranging
gives
$$ E_t y_{t+j} = \alpha_1^{j-1} \Bigl[ {\alpha_1 + {\gamma_1 \over \gamma_0}
\over 1 + {\gamma_1 \over \gamma_0} L}\Bigr]\, y_t  . $$
As $\alpha_1 \to 1$ from below, this formula becomes
$$ E_t y_{t+j} = \Bigl[ {1 + {\gamma_1 \over \gamma_0} \over 1 +
{\gamma_1 \over \gamma_0} L}\Bigr]\, y_t  , \EQN ob44 $$
which is independent of the forecast horizon $j$.  In the limiting case
$\alpha_1 = 1$, it is optimal to forecast $y_t$ for any horizon as a geometric
distributed lag of past $y$'s.  This is Muth's finding that a univariate
process whose first difference is a first-order moving average is optimally
forecast via an ``adaptive expectations'' scheme (i.e., a geometric distributed
lag with the weights adding up to unity).

\subsection
{Growth}

In much of our analysis, we assume that the
eigenvalues of $A$ have absolute values less than or equal to one.
We have seen that such a restriction still allows for polynomial growth.
Geometric growth can also be accommodated by suitably scaling the state
vector.  For instance, suppose that $\{x_t^+ : t=1,2,\ldots\}$
satisfies
$$x_{t+1}^+ = A^+ x_t^+ + C w_{t+1}^+ , \EQN ob45 $$
where $E(w_{t+1}^+ \mid J_t) = 0$ and $E[w_{t+1}^+ (w_{t+1}^+)^\prime \mid
J_t] = (\varepsilon)^t I$.  The positive number $\varepsilon$ can be bigger
than one.  The eigenvalues of $A^+$ are assumed to have absolute values that
are less than or equal to $\varepsilon^{1\over 2}$, an assumption that we make
to ensure that the matrix $A = \varepsilon^{- {1\over 2}} A^+$  has eigenvalues with
modulus bounded above by unity.  We transform variables as follows:
$$x_t = (\varepsilon)^{- {t\over 2}} x_t^+ \EQN ob46 $$
$$w_t = (\varepsilon)^{- {t\over 2}} w_t^+ . \EQN ob47 $$
The transformed process $\{w_t : t=1,2,\ldots\}$ is now conditionally
homoscedastic as required because $E[w_{t+1}(w_{t+1})^\prime \mid J_t] = I$.
Furthermore, the transformed process $\{x_t : t=1,2,\ldots\}$ satisfies
\Ep{ob1}.
%with $A = \varepsilon^{- {1\over 2}} A^+$.
 The matrix $A$ now satisfies the
restriction that its eigenvalues are bounded in modulus by unity.  The original
process $\{x_t^+ : t=1,2,\ldots\}$ is allowed to grow over time at a rate of up
to .5 log $(\varepsilon)$.

\subsection
{A Rational Expectations Model}

Consider a stochastic process  $\{p_t\}$  related to a stochastic process $\{m_t\}$ via
$$p_t = \lambda E_t p_{t+1} + \gamma m_t\, , \quad  0 < \lambda < 1
\EQN ob48 $$
where
$$ m_t = Gx_t \EQN ob49 $$
and $x_t$ is governed by \Ep{ob1}.  In \Ep{ob48}, $E_t (\cdot)$ denotes $E (\cdot)
\mid J_t$. This is a rational expectations version of
 Cagan's (1956) \auth{Cagan, Phillip}%
 model of \index{rational expectations!hyperinflation model}%
 \index{rational expectations!stock price model}%
hyperinflation, where $p_t$ is the log of the price level and $m_t$ the log of
the money supply,  or a version of LeRoy and Porter's (1981)
 \auth{Porter, Richard}%
\auth{LeRoy, Stephen}%
 and Shiller's (1981) model \auth{Shiller, Robert}%
  of stock
prices, where $p_t$ is the stock price and $m_t$ is the dividend.
Recursions on  the difference equation \Ep{ob48} establish that a solution is % to \Ep{ob48} is
$p_t = E_t \gamma \sum_{j=0}^\infty \lambda^j m_{t+j} . $
Using \Ep{ob7} and \Ep{ob49} in this equation gives
$p_t =  \gamma G \sum_{j=0}^\infty \lambda^j A^j x_t ,$
or
$p_t = \gamma G (I - \lambda A)^{-1} x_t  . $
Collecting our results, we have that $(p_t, m_t)$ satisfies
$$\eqalign{ \left[\matrix{ p_t \cr m_t \cr}\right] &= \left[\matrix{\gamma
G(I-\lambda A)^{-1} \cr G \cr}\right]\ x_t \cr
x_{t+1} &= Ax_t + C w_{t+1} .\cr} \EQN ob51 $$
System \Ep{ob51} embodies the cross-equation restrictions associated with
rational expectations models: note that the same parameters in $A,G$
that pin down the stochastic process for $m_t$ also enter the equation
that determines $p_t$ as a function of the state $x_t$.
\index{cross-equation restrictions!rational expectations}

\subsection{Method of Undetermined Coefficients}
It is useful to show how to derive \Ep{ob51} using the {\it method of
undetermined coefficients}.  Returning to \Ep{ob48}, we guess that a
solution for $p_t$ is of the form
$p_t = Hx_t$,
where $H$ is a matrix to be determined.  Given this guess and \Ep{ob1}, it
follows that
$ E_t p_{t+1} = HE_t x_{t+1} = H Ax_t .$
Substituting this and \Ep{ob49} into \Ep{ob48} gives
$ Hx_t = \lambda HA x_t + \gamma G x_t , $
which must hold for all realizations $x_t$. This implies that
$ H = \lambda HA  + \gamma G $
or
$ H = \gamma G (I - \lambda A)^{-1}$,
which agrees with \Ep{ob51}.

%\section
%{Spectral Density Matrix}
%\index{spectral density matrix}\index{autocovariance function}%
%Let the mean vector of $x_t$ from the stationary distribution of an
%$\{ x_t \}$ process be denoted $\mu$.  Define the {\it autocovariance function\/}
%of the $\{ x_t \}$ process as $C_x (\tau) = E [x_t - \mu ]\ [ x_{t - \tau}
%- \mu]^\prime$.  The {\it spectral density matrix\/} of the $\{x_t\}$ process
%is defined as\NFootnote{See Sargent (1987b, chapter XI) for an exposition
%of spectral densities and their uses.}
%$$ S_x (\omega) = \sum^\infty _{\tau = - \infty} C_x (\tau) e ^{- i
%\omega \tau} , \ \  \omega \in [-\pi, \pi].  \EQN ob52 $$
%Consider an $\{x_t\}$ process governed by \Ep{ob1}, where  $x_t$ is partitioned
%as in equations \Ep{ob27},\Ep{ob28}, so that $x_{2t}$ is the constant term.  Then
%the spectral density can be represented as
%$$S_x (\omega) = (I-A_{11} e^{-i \omega})^{-1} C_1C_1^\prime\,
%(I - A_{11}^\prime e^{+ i \omega})^{-1} . \EQN ob53 $$
%From $S_x (\omega)$,
% the autocovariances can be recovered via the inversion formula\NFootnote{The MATLAB program
%{\tt spectral.m} can be used to  \mtlb{spectral.m}
%compute a spectral density matrix. The program requires that the position
%of the constant term, denoted {\tt nnc}, in $x_t$ be specified.  The
%program then forms the appropriate matrices $A_{11}$ and $C_1$ in
%equations \Ep{ob27},\Ep{ob28}, and applies formula \Ep{ob53}.}
%$$C_x (\tau) = \Bigl( {1 \over 2 \pi}\Bigr)\, \int_{-\pi}^\pi S_x (\omega)
%e^{+ i\omega\tau}\, d\omega . \EQN ob54 $$
%These formulas enable us to compute the spectral and cross-spectral
%statistics for the large variety of models that are
%special cases of \Ep{ob1}.



% \input computed_examples_chapter_2.tex % I have removed the computed examples
\section
{Concluding Remarks}

Chapters \use{econenv}, \use{planprob}, \use{commodity}, \use{ge}, and \use{applications}
describe a class of economic structures
with competitive equilibrium prices and quantities  that can be
represented in terms of a vector linear stochastic difference equation.
In particular, the {\it state\/} of the economy $x_t$ will be represented by a
version of \Ep{ob1}, while a vector
$y_t$ containing various prices and quantities will simply be linear
functions of the state, that is, $y_t = G x_t$.
The rest of this book studies how the  matrices $A,C,G$ can
be interpreted  as functions of  parameters that determine the
preferences, technologies, and information flows.
%
%\input msch1  % Introduction\label{intro}
%\input msch2  % Stochastic Linear Dif\hbox{f}erence Equations\label{stodiff}
%\input msch3  % The Economic Environment\label{econenv}
%\input msch4  % Optimal Resource Allocation\label{planprob}}
%\input msch5  % The Commodity Space\label{commodity}
%\input msch6  % A Competitive Economy\label{ge}
%\input msch7  % Applications\label{applications} 