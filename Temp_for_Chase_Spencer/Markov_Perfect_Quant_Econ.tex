\section{Markov perfect equilibrium}

It is instructive to consider a dynamic model of duopoly.
A  market has two firms. Each firm recognizes that its
output decision will affect the aggregate output and therefore influence the
market price. Thus, we drop the assumption of price-taking behavior.\NFootnote{One
consequence of departing from the price-taking framework is that the market
outcome will no longer maximize welfare, measured as the sum of consumer and
producer surplus. See exercise {\it \the\chapternum.4\/} for the case of a monopoly.}
The  one-period return
function of firm $i$ is
$$ R_{it} = p_t y_{it} - .5 d (y_{i t+1} - y_{it})^2.  \EQN game1 $$
There is a demand curve
$$ p_t = A_0 - A_1 (y_{1t} +  y_{2t}) .  \EQN game2 $$
Substituting the demand curve into equation \Ep{game1} lets us express
the return as
$$ R_{it} = A_0 y_{it} - A_1 y_{it}^2 - A_1 y_{it}y_{-i,t}
     - .5 d (y_{it+1} - y_{it})^2 , \EQN game3 $$
where $y_{-i,t}$ denotes the output of the firm other than $i$.
Firm $i$  chooses a decision rule that sets
$y_{it+1}$ as a function of $(y_{it}, y_{-i,t})$  and that  maximizes
$$ \sum_{t=0}^\infty \beta^t R_{it} .$$
Temporarily assume that  the maximizing
decision rule is $y_{it+1}  = f_i(y_{it}, y_{-i,t})$.


Given the function $f_{-i}$, the Bellman equation
of firm $i$ is
$$ v_i(y_{it}, y_{-i,t}) = \max_{y_{it+1}} \left\{
    R_{it} + \beta v_i(y_{it+1}, y_{-i,t+1}) \right\},  \EQN game4 $$
%or
%$$ v_i(y_{it}, y_{-i,t}) = \max_{y_{it+1}} \left\{ (A_0 y_{it} - A_1 y_{it}^2 - A_1 y_{it}y_{-i,t}
%     - .5 d (y_{it+1} - y_{it})^2 ) + \beta v_i(y_{it+1}, y_{-i,t+1}) \right\},  $$
where the maximization is subject to the perceived
decision rule of the other firm
$$ y_{-i,t+1} = f_{-i}(y_{-i,t}, y_{it}). \EQN game5 $$
Note the cross-reference between the two problems for $i=1,2$.

We now advance  the following definition:
\medskip
\specsec{Definition}:
 A Markov perfect equilibrium is a  pair of  value
functions $v_i$ and a pair of policy functions
$f_i$ for $i=1,2$ such that
\medskip
\noindent{\bf a.} Given $f_{-i}$,$v_i$ satisfies
the Bellman equation  \Ep{game4}.
\medskip
\noindent{\bf b.}  The policy function $f_i$ attains the right side
of the Bellman equation \Ep{game4}.
\medskip

  The adjective Markov denotes that the equilibrium decision
rules depend on the current values of the state variables
$y_{it}$ only, not other parts of their histories.   Perfect means `complete', i.e., that
the equilibrium is constructed by backward induction and therefore
builds in optimizing behavior for each firm for all possible
future states, including many that will  not be realized when we iterate
 forward on the pair of equilibrium
strategies $f_i$.

\subsection{Computation}

  If it exists, a Markov perfect equilibrium can be computed
by iterating to convergence on the pair of Bellman equations
\Ep{game4}.  In particular, let $v_i^j,f_i^j$ be the value function
and policy function for firm $i$ at the $j$th iteration.  Then
imagine constructing the iterates
$$ v_i^{j+1}(y_{it}, y_{-i,t}) = \max_{y_{i,t+1}} \left\{
    R_{it} + \beta v_i^{j}(y_{it+1}, y_{-i,t+1}) \right\}, \EQN game4 $$
where the maximization is subject to
$$ y_{-i,t+1} = f^j_{-i}(y_{-i,t}, y_{it}). \EQN game5 $$

  In general, these iterations are difficult.\NFootnote{See Levhari and
Mirman (1980) for how a Markov perfect equilibrium can be computed
conveniently with logarithmic returns and Cobb-Douglas transition
laws. Levhari and Mirman
construct a model of fish and fishers.}
In the next section, we describe how
the calculations simplify for the case in which
the return function is quadratic and the transition laws are linear.
\index{Markov perfect equilibrium!fish and fishers}
\auth{Mirman, Leonard J.}
\auth{Levhari, David}\index{optimal linear regulator!dynamic game}
\index{Markov perfect equilibrium!linear}
\section{Linear Markov perfect equilibria}
In this section, we show how the optimal linear regulator
can be used to solve a model like that in the previous section.
That model should be considered to be
 an example of a dynamic game.  A dynamic  game consists
of these objects: (a) a list of players; (b) a list of dates and
actions available to each player at each date; and (c) payoffs
for each player expressed as  functions of the actions taken
by all players.

  The optimal linear regulator is a good tool for formulating  and solving
dynamic games.  The standard equilibrium
concept---subgame perfection---in these games requires
that each player's strategy be computed
by backward induction.   This leads to an interrelated pair
of Bellman equations. In linear quadratic dynamic games, these
``stacked Bellman equations'' become ``stacked Riccati equations''
with a tractable mathematical structure.
\index{Bellman equation!stacked}
\index{Riccati equation!stacked}
\index{linear quadratic!dynamic games}

We now consider the following two-player, linear quadratic {\it dynamic
game}.  An $(n \times 1)$ state vector $x_t$ evolves according to a
transition equation
$$x_{t+1} = A_t x_t + B_{1t} u_{1t} + B_{2t} u_{2t}  \EQN orig-0$$
where
 $ u_{jt}$ is a $(k_j \times 1)$ vector of controls of
player  $j$.  We start with a finite horizon formulation, where
$t_0$ is the initial date and $t_1$ is the terminal date
for the common horizon of the two players.
Player $1$ maximizes
$$ - \sum_{t=t_0}^{t_1 - 1}  \left( x_t^T R_1 x_t + u_{1t}^T Q_1 u_{1t} +
u_{2t}^T S_1 u_{2t}\right) \EQN orig-1 $$
where $R_1$ and $S_1$ are positive semidefinite and
 $Q_1$ is positive definite.
Player 2 maximizes
$$ - \sum_{t=t_0}^{t_1 - 1} \left( x_t^T R_2 x_t + u_{2t}^T Q_2 u_{2t} +
u_{1t}^T S_2 u_{1t} \right) \EQN orig-2 $$
where $R_2$ and $S_2$ are positive semidefinite and $Q_2$ is positive definite.
\index{Markov perfect equilibrium}

We formulate a Markov perfect equilibrium as follows.  Player
$j$ employs linear decision rules
$$u_{jt} = - F_{jt}  x_t, \ \ t = t_0, \ldots, t_1 - 1$$
where $F_{jt}$ is a $(k_j \times n)$ matrix.  Assume that  player $i$ knows
$\{F_{-i,t}; t = t_0, \ldots, t_1 - 1 \}$.  Then player  1's problem is to
maximize expression
 \Ep{orig-1} subject to the known law of motion \Ep{orig-0}
{\it and\/}
the known control law $u_{2t} = - F_{2t} x_t$ of player 2.
Symmetrically, player 2's problem is to maximize expression
\Ep{orig-2} subject
to equation \Ep{orig-0} and $u_{1t} = - F_{1t} x_t$. A Markov perfect
equilibrium is a pair of
sequences $\{F_{1t}, F_{2t};\, t = t_0, t_0 + 1 , \ldots,
t_1 - 1 \}$ such that $\{F_{1t}\}$ solves player 1's problem, given
$\{F_{2t}\}$, and $\{F_{2t}\}$ solves player 2's problem, given $\{F_{1t}\}$.
We have restricted each player's strategy to depend
only on $x_t$, and not on the {\it history} $h_t =\{(x_s, u_{1s}, u_{2s}),
s = t_0, \ldots, t\}$.
 This restriction on strategy spaces
 accounts for the adjective
``Markov'' in the phrase ``Markov perfect equilibrium.''

Player 1's problem is to maximize
$$ - \sum_{t=t_0}^{t_1 - 1}\Bigl\{ x_t^T (R_1 + F_{2t}^T S_1 F_{2t}) x_t
+ u_{1t}^T Q_1 u_{1t} \Bigr\} $$
subject to
$$x_{t+1} = (A_t- B_{2t} F_{2t}) x_t + B_{1t} u_{1t}. %% + \xi_{1t+1} .
$$
This is an \idx{optimal linear regulator} problem, and it can be solved
by working backward. Evidently, player 2's problem is also an
optimal linear regulator problem.


The solution of player 1's problem is given by
$$F_{1t} = ( B_{1t}^T P_{1t+1} B_{1t} + Q_1)^{-1}  B_{1t}^T P_{1t+1}
(A_t - B_{2t} F_{2t}) \EQN orig-3$$
$$t = t_0, t_0 + 1 , \ldots, t_1 - 1$$
where $P_{1t}$ is the solution of the following matrix Riccati difference
equation with terminal condition $P_{1t_{1}} = 0$:
{ \ninepoint
$$\eqalign{& \, P_{1t}
 = (A_t - B_{2t} F_{2t})^T P_{1t+1} (A_t - B_{2t} F_{2t}) +
 (R_1 +  F_{2t}^T S_1 F_{2t}) \cr
 - &(A_t   - B_{2t} F_{2t})^T P_{1t+1} B_{1t} (B_{1t}^T P_{1t+1} B_{1t} +
Q_1)^{-1} B_{1t}^T P_{1t+1} (A_t - B_{2t} F_{2t}).\cr} \EQN orig-4$$
} % endninepoint
The solution of player 2's problem is
$$F_{2t} = (B_{2t}^T P_{2t+1} B_{2t} + Q_2)^{-1} B_{2t}^T P_{2t+1} (A_t -
B_{1t} F_{1t}) \EQN orig-5$$
where $P_{2t}$ solves the following matrix Riccati difference
equation, with terminal condition $P_{2t_1} = 0$:
$$\eqalign {P_{2t} &= (A_t - B_{1t} F_{1t})^T P_{2t+1} (A_t - B_{1t} F_{1t}) +
(R_2 + F_{1t}^T S_2 F_{1t}) \cr
&- (A_t - B_{1t} F_{1t})^T P_{2t+1} B_{2t} \cr & (B_{2t}^T P_{2t+1} B_{2t} +
Q_2)^{-1} B_{2t}^T P_{2t+1} (A_t - B_{1t} F_{1t}).\cr} \EQN orig-6$$

 The equilibrium sequences $\{F_{1t}, F_{2t}; t =
t_0, t_0 + 1 , \ldots, t_1 - 1\}$ can be calculated from
the pair of coupled Riccati difference equations \Ep{orig-4} and \Ep{orig-6}.
In particular, we  use equations  \Ep{orig-3},
\Ep{orig-4}, \Ep{orig-5}, and \Ep{orig-6} to ``work backward'' from time
$t_1 - 1$.  Notice that given $P_{1t+1}$
and $P_{2t+1}$, equations \Ep{orig-3} and \Ep{orig-5} are a system of
$(k_2 \times n) + (k_1
\times n)$ {\it linear\/} equations in the $(k_2 \times n) + (k_1 \times n)$
unknowns in the matrices $F_{1t}$ and $F_{2t}$.

 Notice how $j$'s control law $F_{jt}$ is a function of $\{F_{is},
s \geq t, i \neq j \}$. Thus, agent $i$'s choice of $\{F_{it}; t = t_0, \ldots,
t_1 - 1\}$ influences agent $j$'s choice of control laws. However, in the
Markov perfect
equilibrium of this game, each agent is assumed
to ignore the influence that his choice exerts on the other agent's
choice.\NFootnote{In an equilibrium of a {\it Stackelberg\/} or {\it dominant
player\/} game, the timing of moves is so altered relative to the present game
that one of the agents called the {\it leader\/} takes
into account the influence that his choices exert on the other agent's
choices. See chapter \use{stackel}.}

  We often want to compute the solutions of such  games for
infinite horizons, in the hope that the decision rules
$F_{it}$ settle down to be time invariant as $t_1 \rightarrow +\infty$.
In practice, we usually  fix $t_1$ and compute the equilibrium
of an infinite horizon game by driving $t_0 \rightarrow - \infty$.   Judd
followed that procedure in the
following example.
\auth{Judd, Kenneth L.}

\subsection{An example}

This section describes the Markov perfect equilibrium of an
infinite horizon linear quadratic game proposed
by Kenneth Judd (1990).
\index{Markov perfect equilibrium!prices and inventory example}%
  The equilibrium is computed by iterating to convergence on the
pair of Riccati equations defined by the choice problems of
two firms.
  Each firm solves a linear quadratic optimization
problem, taking as given and known the sequence of linear decision rules
used by the other player.  The firms set prices and quantities of two
goods interrelated through their demand curves.
  There is no uncertainty.  Relevant variables are defined
as follows:
\smallskip
\item{} $I_{it}$ = inventories of firm $i$ at beginning of $t$.
\item{} $q_{it}$ = production of firm $i$ during period $t$.
\item{} $p_{it}$ = price charged by firm $i$ during period $t$.
\item{} $S_{it}$ = sales made by firm $i$ during period $t$.
\item{} $E_{it}$ = costs of production of firm $i$ during period $t$.
\item{}  $C_{it}$ = costs of carrying inventories for firm $i$ during $t$.
\medskip
\noindent The firms' cost functions are
\smallskip
\item{} $C_{it} = c_{i1} + c_{i2} I_{it} + .5 c_{i3} I_{it}^2$
\item{} $E_{it} = e_{i1} + e_{i2}q_{it} + .5 e_{i3} q_{it}^2$
\smallskip
\noindent where $e_{ij},c_{ij}$ are positive scalars.

\noindent Inventories obey the laws of motion
$$ I_{i,t+1} = (1 - \delta)  I_{it} + q_{it} - S_{it} $$
Demand is governed by the linear schedule
$$S_t = d p_{it} + B$$
where $S_t = \left[\matrix{S_{1t} & S_{2t}\cr}\right]'$,
 $d$ is a $(2\times 2)$ negative definite matrix, and
$B$ is a
vector of constants.  Firm $i$ maximizes the undiscounted sum
$$\lim_{T \to \infty}\ {1 \over T}\   \sum^T_{t=0}\   \left(
p_{it} S_{it} - E_{it} - C_{it} \right) $$
by choosing a decision rule for price and quantity of the form
$$u_{it} = -F_i  x_t$$
where $u_{it} =\left[ \matrix{p_{it} & q_{it}\cr}\right]'$,
 and the state is $x_t=\left[\matrix{I_{1t} & I_{2t}\cr}\right]$.

\mtlb{nnash.m}
\mtlb{nash.m}
In the web site for the book,
we supply a Matlab program {\tt nnash.m} that
computes a Markov perfect equilibrium of
the linear quadratic dynamic game
in which player $i$ maximizes
$$ - \sum_{t=0}^\infty \{ x_t' r_i x_t + 2 x_t' w_i u_{it} +u_{it}' q_i
  u_{it} + u_{jt}' s_i u_{jt} + 2 u_{jt}' m_i u_{it} \}$$
subject to the law of motion
$$x_{t+1} = a x_t + b_1 u_{1t}+b_2 u_{2t}$$
and a control law $u_{jt}= -f_j x_t$ for the other player; here
$a$ is $n \times n$; $b_1$ is $n \times k_1$; $b_2$ is $n \times k_2$;
$r_1$ is $n\times n$; $r_2$ is $ n \times n$;
$q_1$ is $k_1 \times k_1$; $q_2$ is $k_2 \times k_2$;
$s_1$ is $k_2 \times k_2$; $s_2$ is $k_1 \times k_1$;
$w_1$ is  $n \times k_1$;
$w_2$ is $n \times k_2$;
$m_1$ is $ k_2 \times k_1$; and  $m_2$ is $k_1 \times k_2$.
The equilibrium of Judd's model can be computed by
filling in the matrices appropriately.  A Matlab tutorial
{\tt judd.m} uses {\tt nnash.m} to compute the equilibrium.